{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-30T06:41:21.093999Z",
     "start_time": "2024-06-30T06:41:09.167524Z"
    }
   },
   "source": [
    "!pip install torchmetrics\n",
    "!pip install omegaconf\n",
    "!pip install wandb\n",
    "!pip install einops\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "from omegaconf import DictConfig\n",
    "import wandb\n",
    "from termcolor import cprint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from termcolor import cprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\romas\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py:2664: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T06:41:24.000994Z",
     "start_time": "2024-06-30T06:41:23.989840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ThingsMEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str, data_dir: str = \"data\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert split in [\"train\", \"val\", \"test\"], f\"Invalid split: {split}\"\n",
    "        self.split = split\n",
    "        self.num_classes = 1854\n",
    "\n",
    "        self.X = torch.load(os.path.join(data_dir, f\"{split}_X.pt\"))\n",
    "        self.subject_idxs = torch.load(os.path.join(data_dir, f\"{split}_subject_idxs.pt\"))\n",
    "\n",
    "        if split in [\"train\", \"val\"]:\n",
    "            self.y = torch.load(os.path.join(data_dir, f\"{split}_y.pt\"))\n",
    "            assert len(torch.unique(self.y)) == self.num_classes, \"Number of classes do not match.\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if hasattr(self, \"y\"):\n",
    "            return self.X[i], self.y[i], self.subject_idxs[i]\n",
    "        else:\n",
    "            return self.X[i], self.subject_idxs[i]\n",
    "\n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        return self.X.shape[2]"
   ],
   "id": "75d43c3508b52006",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class BasicConvClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        seq_len: int,\n",
    "        in_channels: int,\n",
    "        hid_dim: int = 128\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            ConvBlock(in_channels, hid_dim),\n",
    "            ConvBlock(hid_dim, hid_dim),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            Rearrange(\"b d 1 -> b d\"),\n",
    "            nn.Linear(hid_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X ( b, c, t ): _description_\n",
    "        Returns:\n",
    "            X ( b, num_classes ): _description_\n",
    "        \"\"\"\n",
    "        X = self.blocks(X)\n",
    "\n",
    "        return self.head(X)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        kernel_size: int = 3,\n",
    "        p_drop: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        self.conv1 = nn.Conv1d(out_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        # self.conv2 = nn.Conv1d(out_dim, out_dim, kernel_size) # , padding=\"same\")\n",
    "\n",
    "        self.batchnorm0 = nn.BatchNorm1d(num_features=out_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features=out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if self.in_dim == self.out_dim:\n",
    "            X = self.conv0(X) + X  # skip connection\n",
    "        else:\n",
    "            X = self.conv0(X)\n",
    "\n",
    "        X = F.gelu(self.batchnorm0(X))\n",
    "\n",
    "        X = self.conv1(X) + X  # skip connection\n",
    "        X = F.gelu(self.batchnorm1(X))\n",
    "\n",
    "        # X = self.conv2(X)\n",
    "        # X = F.glu(X, dim=-2)\n",
    "\n",
    "        return self.dropout(X)"
   ],
   "id": "728f66b12c3aa7e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BasicLSTMClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int, seq_len: int, in_channels: int, hid_dim: int = 50, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.batch_norm_input = nn.BatchNorm1d(seq_len)\n",
    "        self.lstm = nn.LSTM(in_channels, hid_dim, 1, batch_first=True, bidirectional=True)\n",
    "        self.batch_norm_lstm = nn.BatchNorm1d(hid_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hid_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = X.permute(0, 2, 1)  # Reorder dimensions to [batch, seq_len, in_channels]\n",
    "        X = self.batch_norm_input(X)\n",
    "        X, _ = self.lstm(X)  # Unpack the tuple returned by LSTM\n",
    "        X = self.batch_norm_lstm(X[:, -1, :])  # Apply batch normalization and use the output from the last time step\n",
    "        X = self.dropout(X)  # Apply dropout\n",
    "        X = self.linear(X)\n",
    "        return X"
   ],
   "id": "5ab5499e08f4ff7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleGRUClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int, seq_len: int, in_channels: int, hid_dim: int = 500, dropout: float = 0.5):\n",
    "        super(SimpleGRUClassifier, self).__init__()\n",
    "        self.batch_norm_input = nn.BatchNorm1d(seq_len)\n",
    "        self.gru = nn.GRU(in_channels, hid_dim, batch_first=True, bidirectional=True)\n",
    "        self.batch_norm_gru = nn.BatchNorm1d(hid_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hid_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = X.permute(0, 2, 1)  # Reorder dimensions to [batch, seq_len, in_channels]\n",
    "        X = self.batch_norm_input(X)\n",
    "        X, _ = self.gru(X)  # Unpack the tuple returned by GRU\n",
    "        X = self.batch_norm_gru(X[:, -1, :])  # Apply batch normalization and use the output from the last time step\n",
    "        X = self.dropout(X)  # Apply dropout\n",
    "        X = self.linear(X)\n",
    "        return X"
   ],
   "id": "eeb1995d57a22341"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "278f33d8f4f20c9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "zip_file_path = '/content/drive/MyDrive/Colab Notebooks/DL_basic_final/data.zip'\n",
    "\n",
    "save_path = '/content/MEG_data/data'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(save_path)"
   ],
   "id": "7eef51c62b283846"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T06:41:25.260913Z",
     "start_time": "2024-06-30T06:41:25.244649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed: int = 0) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ],
   "id": "4d2e22f93927655b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\" : 80,\n",
    "    \"lr\" : 0.001,\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"num_workers\" : 4,\n",
    "    \"seed\" : 1234,\n",
    "    \"use_wandb\" : True,\n",
    "    \"data_dir\" : '/content/MEG_data/data',\n",
    "}"
   ],
   "id": "3b45a67604c2e62a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T06:41:25.617774Z",
     "start_time": "2024-06-30T06:41:25.603023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run(args: DictConfig):\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    logdir = '/content/MEG_data/log'\n",
    "\n",
    "    # ------------------\n",
    "    #    Dataloader\n",
    "    # ------------------\n",
    "    loader_args = {\"batch_size\": args.batch_size, \"num_workers\": args.num_workers}\n",
    "\n",
    "    train_set = ThingsMEGDataset(\"train\", args.data_dir)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, **loader_args)\n",
    "    val_set = ThingsMEGDataset(\"val\", args.data_dir)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, shuffle=False, **loader_args)\n",
    "    test_set = ThingsMEGDataset(\"test\", args.data_dir)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers\n",
    "    )\n",
    "\n",
    "    # ------------------\n",
    "    #       Model\n",
    "    # ------------------\n",
    "    model = SimpleGRUClassifier(\n",
    "        train_set.num_classes, train_set.seq_len, train_set.num_channels, dropout=0.5\n",
    "    ).to(args.device)\n",
    "\n",
    "    # ------------------\n",
    "    #     Optimizer\n",
    "    # ------------------\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,weight_decay=1e-5)\n",
    "\n",
    "    # ------------------\n",
    "    # Learning Rate Scheduler\n",
    "    # ------------------\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "    # ------------------\n",
    "    #   Start training\n",
    "    # ------------------\n",
    "    max_val_acc = 0\n",
    "    accuracy = Accuracy(\n",
    "        task=\"multiclass\", num_classes=train_set.num_classes, top_k=10\n",
    "    ).to(args.device)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "\n",
    "        train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "        model.train()\n",
    "        for X, y, subject_idxs in tqdm(train_loader, desc=\"Train\"):\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "            y_pred = model(X)\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = accuracy(y_pred, y)\n",
    "            train_acc.append(acc.item())\n",
    "\n",
    "        model.eval()\n",
    "        for X, y, subject_idxs in tqdm(val_loader, desc=\"Validation\"):\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X)\n",
    "\n",
    "            val_loss.append(F.cross_entropy(y_pred, y).item())\n",
    "            val_acc.append(accuracy(y_pred, y).item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs} | train loss: {np.mean(train_loss):.3f} | train acc: {np.mean(train_acc):.3f} | val loss: {np.mean(val_loss):.3f} | val acc: {np.mean(val_acc):.3f}\")\n",
    "        torch.save(model.state_dict(), os.path.join(logdir, \"model_last.pt\"))\n",
    "\n",
    "        if np.mean(val_acc) > max_val_acc:\n",
    "            cprint(\"New best.\", \"cyan\")\n",
    "            torch.save(model.state_dict(), os.path.join(logdir, \"model_best.pt\"))\n",
    "            max_val_acc = np.mean(val_acc)\n",
    "\n",
    "\n",
    "    # ----------------------------------\n",
    "    #  Start evaluation with best model\n",
    "    # ----------------------------------\n",
    "    model.load_state_dict(torch.load(os.path.join(logdir, \"model_best.pt\"), map_location=args.device))\n",
    "\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    for X, subject_idxs in tqdm(test_loader, desc=\"Validation\"):\n",
    "        preds.append(model(X.to(args.device)).detach().cpu())\n",
    "\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    np.save(os.path.join(logdir, \"submission\"), preds)\n",
    "    cprint(f\"Submission {preds.shape} saved at {logdir}\", \"cyan\")\n"
   ],
   "id": "641c54e56570d062",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-30T06:41:26.789614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = DictConfig(config)\n",
    "run(args)"
   ],
   "id": "612a638a8ef8ba9f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
